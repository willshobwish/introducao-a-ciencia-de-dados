{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from optuna.study import MaxTrialsCallback\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e57f159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_booster().attributes())\n",
    "# print(model.get_params())\n",
    "# print(model.get_all_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7595cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"predict_students_dropout_and_academic_success.csv\",delimiter=\";\")\n",
    "le = LabelEncoder()\n",
    "df[\"Target\"] = le.fit_transform(df[\"Target\"])\n",
    "\n",
    "# Assume df has features and df_encoded[\"Target\"] is the encoded label\n",
    "X = df.iloc[:,:-1]\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Optional: split to test performance later\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef89b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_xgb_gpu(model):\n",
    "    attrs = model.get_booster().attributes()\n",
    "    gpu_used = False\n",
    "    if 'gpu_id' in attrs:\n",
    "        print(f\"XGBoost GPU ID used: {attrs['gpu_id']}\")\n",
    "        gpu_used = True\n",
    "    elif 'device' in attrs and 'gpu' in attrs['device']:\n",
    "        print(f\"XGBoost device: {attrs['device']}\")\n",
    "        gpu_used = True\n",
    "    else:\n",
    "        print(\"XGBoost GPU not used.\")\n",
    "    return gpu_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "023f48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_lgb_gpu(model):\n",
    "    # LightGBM prints info if verbose is set, but we can inspect params\n",
    "    params = model.get_params()\n",
    "    if 'device' in params and params['device'] == 'gpu':\n",
    "        print(\"LightGBM set to use GPU.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"LightGBM GPU not used.\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d337112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cat_gpu(model):\n",
    "    params = model.get_params()\n",
    "    if params.get('task_type', '').lower() == 'gpu':\n",
    "        print(\"CatBoost set to use GPU.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CatBoost GPU not used.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e36a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, X_test, y_test, name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Only call .get() if it's a CuPy array\n",
    "    if isinstance(y_pred, cp.ndarray):\n",
    "        y_pred = y_pred.get()\n",
    "    if isinstance(y_test, cp.ndarray):\n",
    "        y_test = y_test.get()\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"\\n=== {name} Metrics ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9293dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 200\n",
    "max_trials_callback = n_trials * 4\n",
    "storage = \"postgresql://myuser:mypassword@localhost:5432/mydatabase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "577e3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Optuna Study for XGBoost\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 10, 2000),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 100),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "        'subsample': trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        'tree_method': 'hist',  # Use GPU\n",
    "        'device':'cuda',\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train_gpu, y_train_gpu)\n",
    "    return accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 18:20:38,895] A new study created in memory with name: no-name-b570c363-6235-4a58-be71-855bb5a57c8e\n",
      "[I 2025-05-18 18:20:54,701] Trial 2 finished with value: 0.7581920903954802 and parameters: {'n_estimators': 179, 'max_depth': 42, 'learning_rate': 0.07635808800489266, 'subsample': 0.2577664030101571, 'colsample_bytree': 0.7462224702994853}. Best is trial 2 with value: 0.7581920903954802.\n",
      "[I 2025-05-18 18:21:15,864] Trial 0 finished with value: 0.7423728813559322 and parameters: {'n_estimators': 851, 'max_depth': 54, 'learning_rate': 0.19486067825630024, 'subsample': 0.1353140996148693, 'colsample_bytree': 0.5974617573764542}. Best is trial 2 with value: 0.7581920903954802.\n",
      "[I 2025-05-18 18:21:20,227] Trial 3 finished with value: 0.7694915254237288 and parameters: {'n_estimators': 848, 'max_depth': 67, 'learning_rate': 0.3896016033854916, 'subsample': 0.43348123376371617, 'colsample_bytree': 0.43290751614451517}. Best is trial 3 with value: 0.7694915254237288.\n",
      "[I 2025-05-18 18:21:22,910] Trial 4 finished with value: 0.7423728813559322 and parameters: {'n_estimators': 692, 'max_depth': 35, 'learning_rate': 0.39818281239781655, 'subsample': 0.18582460356833663, 'colsample_bytree': 0.43905419926053124}. Best is trial 3 with value: 0.7694915254237288.\n",
      "[I 2025-05-18 18:21:35,877] Trial 1 finished with value: 0.7672316384180791 and parameters: {'n_estimators': 1281, 'max_depth': 85, 'learning_rate': 0.31292856905671473, 'subsample': 0.4701852736977965, 'colsample_bytree': 0.21562724331078023}. Best is trial 3 with value: 0.7694915254237288.\n",
      "[I 2025-05-18 18:22:00,007] Trial 7 finished with value: 0.7107344632768362 and parameters: {'n_estimators': 848, 'max_depth': 23, 'learning_rate': 0.4351071153902439, 'subsample': 0.13968254086339765, 'colsample_bytree': 0.34167451998449216}. Best is trial 3 with value: 0.7694915254237288.\n",
      "[I 2025-05-18 18:22:44,009] Trial 8 finished with value: 0.7649717514124293 and parameters: {'n_estimators': 1653, 'max_depth': 72, 'learning_rate': 0.39501398184987985, 'subsample': 0.47982345675872284, 'colsample_bytree': 0.4547183916582608}. Best is trial 3 with value: 0.7694915254237288.\n"
     ]
    }
   ],
   "source": [
    "X_train_gpu = cp.array(X_train)\n",
    "X_test_gpu = cp.array(X_test)\n",
    "y_train_gpu = cp.array(y_train)\n",
    "y_test_gpu = cp.array(y_test)\n",
    "\n",
    "xgb_study = optuna.create_study(direction=\"maximize\", storage=storage, load_if_exists=True)\n",
    "xgb_study.optimize(xgb_objective, n_trials=n_trials,callbacks=[MaxTrialsCallback(max_trials_callback, states=(TrialState.COMPLETE,))])\n",
    "xgb_best_model = XGBClassifier(**xgb_study.best_params, tree_method='hist', device='cuda', eval_metric='logloss')\n",
    "xgb_best_model.fit(X_train_gpu, y_train_gpu)\n",
    "check_xgb_gpu(xgb_best_model)\n",
    "evaluate_model(xgb_best_model, X_test_gpu, y_test_gpu, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9801cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Optuna Study for LightGBM\n",
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 10, 2000),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 100),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 3, 1000),\n",
    "        'device': 'gpu',  # Use GPU\n",
    "    }\n",
    "    model = LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d067348",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_study = optuna.create_study(direction=\"maximize\", storage=storage, load_if_exists=True)\n",
    "lgb_study.optimize(lgb_objective, n_trials=n_trials)\n",
    "lgb_best_model = LGBMClassifier(**lgb_study.best_params)\n",
    "lgb_best_model.fit(X_train, y_train)\n",
    "check_lgb_gpu(lgb_best_model)\n",
    "evaluate_model(lgb_best_model, X_test, y_test,\"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8602a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Optuna Study for CatBoost\n",
    "def cat_objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int(\"iterations\", 100, 500),\n",
    "        'depth': trial.suggest_int(\"depth\", 3, 16),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "        'loss_function': 'MultiClass',\n",
    "        'task_type': 'CPU',  # Use GPU\n",
    "        'devices':'0'\n",
    "    }\n",
    "    model = CatBoostClassifier(**params, verbose=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    return accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab652d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_study = optuna.create_study(direction=\"maximize\", storage=storage, load_if_exists=True)\n",
    "cat_study.optimize(cat_objective, n_trials=n_trials)\n",
    "cat_best_model = CatBoostClassifier(**cat_study.best_params, verbose=0)\n",
    "cat_best_model.fit(X_train, y_train)\n",
    "check_cat_gpu(cat_best_model)\n",
    "evaluate_model(cat_best_model, X_test, y_test, \"CatBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
